{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "cosmetic-calvin",
   "metadata": {},
   "source": [
    "# Intro to Programming with Spark"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "daily-ending",
   "metadata": {},
   "source": [
    "# References\n",
    "\n",
    "* http://localhost:4040\n",
    "* https://spark.apache.org/docs/latest/api/python/index.html\n",
    "* https://spark.apache.org/docs/latest/api/python/reference/pyspark.sql.html#functions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "computational-sustainability",
   "metadata": {},
   "source": [
    "## Install"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "comparable-authority",
   "metadata": {},
   "source": [
    "**Set-up Instructions:**\n",
    "\n",
    "1. Install Spark (latest)\n",
    "    1. Prebuilt for version\n",
    "2. Install Open JDK (8 or 11) aka Java\n",
    "3. Install Python3\n",
    "4. Pip3 install pyspark\n",
    "5. Set up environment variables\n",
    "    * PYTHONPATH\n",
    "    * PYSPARK_PYTHON\n",
    "    * SPARK_HOME\n",
    "    * JAVA_HOME\n",
    "6. Install jdbc postgres driver for version of Spark\n",
    "    * Save in `/Spark/spark-3.0.1-bin-hadoop3.2/jars/`\n",
    "    \n",
    "**Opening Spark**\n",
    "\n",
    "* Open PySpark in Terminal or PowerShell.\n",
    "    * `pyspark`\n",
    "* To open a jupyter notebook with pyspark, first set these env variables in your terminal or the equivalent commands in PowerShell, before running the pyspark command.\n",
    "    * `export PYSPARK_DRIVER_PYTHON=jupyter` \n",
    "    * `export PYSPARK_DRIVER_PYTHON_OPTS='notebook'`\n",
    "* Run a script with Spark.\n",
    "    * `spark-submit my_filename.py`\n",
    "    * pass packages to spark with `spark-submit --packages package_name`"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "alternate-volume",
   "metadata": {},
   "source": [
    "## Two APIs to Know"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "differential-cheat",
   "metadata": {},
   "source": [
    "The following APIs, along with a number of additional (unmentioned here) Spark APIs, provide a Python programming interface to Spark.\n",
    "\n",
    "* **DataFrame API**\n",
    "    - Spark DataFrame is not the same as a Pandas DataFrame, though it can be changed to one if needed.\n",
    "    \n",
    "* **Spark SQL API**\n",
    "    - We create a temp view (or global view) and query the view.\n",
    "    - The view will persist our changes. No need to keep creating & pulling down the data!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "listed-question",
   "metadata": {},
   "source": [
    "## Transformations versus Actions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "sexual-blink",
   "metadata": {},
   "source": [
    "**Transformations**\n",
    "- Transformations include all:\n",
    "    * selecting\n",
    "    * filtering\n",
    "    * grouping\n",
    "    * summing\n",
    "    * ordering \n",
    "    * etc.\n",
    "\n",
    "**Actions**\n",
    "- Actions include:\n",
    "    * View data\n",
    "    * Collect data\n",
    "    * Write data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "automotive-accuracy",
   "metadata": {},
   "source": [
    "## Spark Context & Session"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "compound-lexington",
   "metadata": {},
   "source": [
    "* **Spark Context:** entry point object to Spark\n",
    "* **Spark Session:** _unified_ entry point object to Spark [2.0+]\n",
    "\n",
    "\n",
    "- In Jupyter notebooks and in pyspark shell, the Session is created for you. In scripts, you must specify it.\n",
    "\n",
    "```\n",
    "from pyspark.sql.session import SparkSession\n",
    "spark = SparkSession.builder.appName('MyApp').getOrCreate()\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aquatic-organ",
   "metadata": {},
   "source": [
    "## 1. Connect to Postgres"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "minus-delicious",
   "metadata": {},
   "outputs": [],
   "source": [
    "# set up user name and password for postgres\n",
    "my_username = ''\n",
    "my_password = ''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "empty-serial",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- id: integer (nullable = true)\n",
      " |-- name: string (nullable = true)\n",
      " |-- address: string (nullable = true)\n",
      " |-- address2: string (nullable = true)\n",
      " |-- city: string (nullable = true)\n",
      " |-- state: string (nullable = true)\n",
      " |-- zipcode: string (nullable = true)\n",
      " |-- date_added: date (nullable = true)\n",
      "\n",
      "+---+---------+---------------+--------+----------+-----+-------+----------+\n",
      "| id|     name|        address|address2|      city|state|zipcode|date_added|\n",
      "+---+---------+---------------+--------+----------+-----+-------+----------+\n",
      "|  1|Location1| 123 Place Road|    null|  New York|   NY|  10001|2021-03-18|\n",
      "|  2|Location2|555 Rock Avenue|    null|    Boston|   MA|  02101|2021-03-18|\n",
      "|  3|Location3|838 Rock Avenue|    null|Pittsburgh|   PA|  15106|2021-03-18|\n",
      "+---+---------+---------------+--------+----------+-----+-------+----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# connect to the database\n",
    "# make sure all args are right & connected to db if stalling\n",
    "offices = spark.read.format(\"jdbc\") \\\n",
    ".option(\"url\", \"jdbc:postgresql://localhost:5433/my_database\") \\\n",
    ".option(\"dbtable\", \"offices\") \\\n",
    ".option(\"driver\", \"org.postgresql.Driver\") \\\n",
    ".option(\"user\" , my_username) \\\n",
    ".option(\"password\", my_password) \\\n",
    ".load()\n",
    "\n",
    "# get a feel for the dataframe\n",
    "offices.printSchema()\n",
    "offices.limit(3).show()\n",
    "\n",
    "# Set up for spark sql API\n",
    "offices.createOrReplaceTempView(\"offices\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "duplicate-photography",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- id: integer (nullable = true)\n",
      " |-- firstname: string (nullable = true)\n",
      " |-- lastname: string (nullable = true)\n",
      " |-- age: integer (nullable = true)\n",
      " |-- title: string (nullable = true)\n",
      " |-- location_id: integer (nullable = true)\n",
      " |-- date_added: date (nullable = true)\n",
      "\n",
      "+---+---------+--------+---+---------------+-----------+----------+\n",
      "| id|firstname|lastname|age|          title|location_id|date_added|\n",
      "+---+---------+--------+---+---------------+-----------+----------+\n",
      "|  1|  Rudolph|Reindeer| 23|  Lead Reindeer|          1|2021-03-18|\n",
      "|  2|   Dasher|Reindeer| 24|Junior Reindeer|          1|2021-03-18|\n",
      "|  3|  Prancer|Reindeer| 25|Junior Reindeer|          2|2021-03-18|\n",
      "+---+---------+--------+---+---------------+-----------+----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "employees = spark.read.format(\"jdbc\") \\\n",
    ".option(\"url\", \"jdbc:postgresql://localhost:5433/my_database\") \\\n",
    ".option(\"dbtable\", \"employees\") \\\n",
    ".option(\"driver\", \"org.postgresql.Driver\") \\\n",
    ".option(\"user\" , my_username) \\\n",
    ".option(\"password\", my_password) \\\n",
    ".load()\n",
    "\n",
    "# get a feel for the dataframe\n",
    "employees.printSchema()\n",
    "employees.limit(3).show()\n",
    "\n",
    "# Set up for spark sql API\n",
    "employees.createOrReplaceTempView(\"employees\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "potential-passion",
   "metadata": {},
   "source": [
    "## 2. Query"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "sonic-colors",
   "metadata": {},
   "source": [
    "**Example Includes:**\n",
    "\n",
    "    * select()\n",
    "    * where() \n",
    "    * isin()\n",
    "    * orderBy()\n",
    "    * show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "norwegian-court",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+\n",
      "|     name|\n",
      "+---------+\n",
      "|Location1|\n",
      "|Location2|\n",
      "+---------+\n",
      "\n",
      "+---------+\n",
      "|     name|\n",
      "+---------+\n",
      "|Location1|\n",
      "|Location2|\n",
      "+---------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# dataframe api\n",
    "# col gives you column manipulation\n",
    "from pyspark.sql.functions import col\n",
    "offices.select(col(\"name\")).where(col(\"name\").isin('Location1','Location2')) \\\n",
    ".orderBy(col(\"name\")) \\\n",
    ".show()\n",
    "\n",
    "# spark sql api\n",
    "# reminder: this is actually querying the \"temp view\"\n",
    "offices_filtered = spark.sql(\"\"\"\n",
    "                    select name \n",
    "                    from offices\n",
    "                    where name in ('Location1','Location2')\n",
    "                    order by name\n",
    "                    \"\"\"\n",
    "                    )\n",
    "offices_filtered.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "patent-challenge",
   "metadata": {},
   "source": [
    "## 3. Join"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "comprehensive-creator",
   "metadata": {},
   "source": [
    "**Example Includes:**\n",
    "\n",
    "* alias() ~ the DataFrame\n",
    "* join()\n",
    "* select()\n",
    "* show() ~ with Truncate = False "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "asian-accident",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+---------+---------------+--------+------------+-----+-------+----------+---+---------+--------+---+------------------+-----------+----------+\n",
      "|id |name     |address        |address2|city        |state|zipcode|date_added|id |firstname|lastname|age|title             |location_id|date_added|\n",
      "+---+---------+---------------+--------+------------+-----+-------+----------+---+---------+--------+---+------------------+-----------+----------+\n",
      "|1  |Location1|123 Place Road |null    |New York    |NY   |10001  |2021-03-18|1  |Rudolph  |Reindeer|23 |Lead Reindeer     |1          |2021-03-18|\n",
      "|1  |Location1|123 Place Road |null    |New York    |NY   |10001  |2021-03-18|2  |Dasher   |Reindeer|24 |Junior Reindeer   |1          |2021-03-18|\n",
      "|3  |Location3|838 Rock Avenue|null    |Pittsburgh  |PA   |15106  |2021-03-18|5  |Comet    |Reindeer|32 |Associate Reindeer|3          |2021-03-18|\n",
      "|3  |Location3|838 Rock Avenue|null    |Pittsburgh  |PA   |15106  |2021-03-18|6  |Cupid    |Reindeer|35 |Senior Reindeer   |3          |2021-03-18|\n",
      "|5  |Location5|210 Windy Road |null    |Minneapolis |MN   |55111  |2021-03-18|9  |Blitzen  |Reindeer|43 |Co-Captain        |5          |2021-03-18|\n",
      "|4  |Location4|321 Pop Street |null    |Philadelphia|PA   |19019  |2021-03-18|7  |Dancer   |Reindeer|37 |Senior Reindeer   |4          |2021-03-18|\n",
      "|4  |Location4|321 Pop Street |null    |Philadelphia|PA   |19019  |2021-03-18|8  |Donner   |Reindeer|38 |Co-Captain        |4          |2021-03-18|\n",
      "|2  |Location2|555 Rock Avenue|null    |Boston      |MA   |02101  |2021-03-18|3  |Prancer  |Reindeer|25 |Junior Reindeer   |2          |2021-03-18|\n",
      "|2  |Location2|555 Rock Avenue|null    |Boston      |MA   |02101  |2021-03-18|4  |Vixen    |Reindeer|28 |Associate Reindeer|2          |2021-03-18|\n",
      "+---+---------+---------------+--------+------------+-----+-------+----------+---+---------+--------+---+------------------+-----------+----------+\n",
      "\n",
      "+---+---------+---------------+--------+------------+-----+-------+----------+---+---------+--------+---+------------------+-----------+----------+\n",
      "|id |name     |address        |address2|city        |state|zipcode|date_added|id |firstname|lastname|age|title             |location_id|date_added|\n",
      "+---+---------+---------------+--------+------------+-----+-------+----------+---+---------+--------+---+------------------+-----------+----------+\n",
      "|1  |Location1|123 Place Road |null    |New York    |NY   |10001  |2021-03-18|1  |Rudolph  |Reindeer|23 |Lead Reindeer     |1          |2021-03-18|\n",
      "|1  |Location1|123 Place Road |null    |New York    |NY   |10001  |2021-03-18|2  |Dasher   |Reindeer|24 |Junior Reindeer   |1          |2021-03-18|\n",
      "|3  |Location3|838 Rock Avenue|null    |Pittsburgh  |PA   |15106  |2021-03-18|5  |Comet    |Reindeer|32 |Associate Reindeer|3          |2021-03-18|\n",
      "|3  |Location3|838 Rock Avenue|null    |Pittsburgh  |PA   |15106  |2021-03-18|6  |Cupid    |Reindeer|35 |Senior Reindeer   |3          |2021-03-18|\n",
      "|5  |Location5|210 Windy Road |null    |Minneapolis |MN   |55111  |2021-03-18|9  |Blitzen  |Reindeer|43 |Co-Captain        |5          |2021-03-18|\n",
      "|4  |Location4|321 Pop Street |null    |Philadelphia|PA   |19019  |2021-03-18|7  |Dancer   |Reindeer|37 |Senior Reindeer   |4          |2021-03-18|\n",
      "|4  |Location4|321 Pop Street |null    |Philadelphia|PA   |19019  |2021-03-18|8  |Donner   |Reindeer|38 |Co-Captain        |4          |2021-03-18|\n",
      "|2  |Location2|555 Rock Avenue|null    |Boston      |MA   |02101  |2021-03-18|3  |Prancer  |Reindeer|25 |Junior Reindeer   |2          |2021-03-18|\n",
      "|2  |Location2|555 Rock Avenue|null    |Boston      |MA   |02101  |2021-03-18|4  |Vixen    |Reindeer|28 |Associate Reindeer|2          |2021-03-18|\n",
      "+---+---------+---------------+--------+------------+-----+-------+----------+---+---------+--------+---+------------------+-----------+----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# dataframe api\n",
    "offices.alias(\"o\")\n",
    "employees.alias(\"e\")\n",
    "join_exp = offices[\"id\"] == employees[\"location_id\"]\n",
    "\n",
    "offices.join(employees, join_exp, \"inner\") \\\n",
    ".select(\"*\") \\\n",
    ".show(50, False)\n",
    "\n",
    "# spark sql api\n",
    "table_join = spark.sql(\"\"\"\n",
    "                    select * \n",
    "                    from offices o\n",
    "                    join employees e\n",
    "                    on o.id = e.location_id\n",
    "                    \"\"\"\n",
    "                    )\n",
    "table_join.show(50, False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "economic-shelter",
   "metadata": {},
   "source": [
    "## 4. Insert"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "million-coordinator",
   "metadata": {},
   "source": [
    "Sorry. **No way to update or delete via Spark.** That's okay, we don't need to use Spark for this. We can, however, insert new records.\n",
    "\n",
    "**Reference** \n",
    "* https://spark.apache.org/docs/2.3.1/sql-programming-guide.html#jdbc-to-other-databases\n",
    "\n",
    "**Example Includes:**\n",
    "\n",
    "* Row()\n",
    "* createDataFrame()\n",
    "* write()\n",
    "* jdbc()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "foreign-testimony",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import Row\n",
    "\n",
    "# dataframe api\n",
    "connect = 'jdbc:postgresql://localhost:5433/my_database'\n",
    "auth = {\"user\": my_username, \"password\": my_password}\n",
    "\n",
    "# create row, set as dF, append dF to dF\n",
    "new_row = [Row('Location6', '441 Rainy Road', 'Unit A', 'Albany', 'NY', '12202')]\n",
    "appendDF = spark.createDataFrame(new_row, offices.schema[1:-1])\n",
    "appendDF.write.jdbc(connect, 'offices', mode=\"append\", properties = auth)\n",
    "\n",
    "# spark sql api\n",
    "# tbd ~ there may be a way"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "synthetic-arthur",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+---------+---------------+--------+------------+-----+-------+----------+\n",
      "| id|     name|        address|address2|        city|state|zipcode|date_added|\n",
      "+---+---------+---------------+--------+------------+-----+-------+----------+\n",
      "|  1|Location1| 123 Place Road|    null|    New York|   NY|  10001|2021-03-18|\n",
      "|  2|Location2|555 Rock Avenue|    null|      Boston|   MA|  02101|2021-03-18|\n",
      "|  3|Location3|838 Rock Avenue|    null|  Pittsburgh|   PA|  15106|2021-03-18|\n",
      "|  4|Location4| 321 Pop Street|    null|Philadelphia|   PA|  19019|2021-03-18|\n",
      "|  5|Location5| 210 Windy Road|    null| Minneapolis|   MN|  55111|2021-03-18|\n",
      "|  6|Location6| 441 Rainy Road|  Unit A|      Albany|   NY|  12202|2021-03-18|\n",
      "+---+---------+---------------+--------+------------+-----+-------+----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "offices.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "adolescent-navigator",
   "metadata": {},
   "source": [
    "## 6. Missing Values (Null/NA)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "offshore-matter",
   "metadata": {},
   "source": [
    "**Example Includes:**\n",
    "\n",
    "* filter()\n",
    "* col()\n",
    "* isNull()\n",
    "* isnan()\n",
    "* show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "valued-substitute",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+----+-------+--------+----+-----+-------+----------+\n",
      "| id|name|address|address2|city|state|zipcode|date_added|\n",
      "+---+----+-------+--------+----+-----+-------+----------+\n",
      "+---+----+-------+--------+----+-----+-------+----------+\n",
      "\n",
      "+---+----+-------+--------+----+-----+-------+----------+\n",
      "|id |name|address|address2|city|state|zipcode|date_added|\n",
      "+---+----+-------+--------+----+-----+-------+----------+\n",
      "+---+----+-------+--------+----+-----+-------+----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import col, count, isnan\n",
    "\n",
    "# dataframe api\n",
    "offices.filter(col(\"name\").isNull() | isnan(col(\"name\"))).show()\n",
    "\n",
    "# spark sql api\n",
    "table_mv = spark.sql(\"\"\"\n",
    "                    select * \n",
    "                    from offices\n",
    "                    where name is null\n",
    "                    \"\"\"\n",
    "                    )\n",
    "table_mv.show(50, False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "northern-dallas",
   "metadata": {},
   "source": [
    "## 7. Cast & Alias"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "million-tokyo",
   "metadata": {},
   "source": [
    "**Example Includes:**\n",
    "\n",
    "* select()\n",
    "* col()\n",
    "* cast()\n",
    "* alias()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "plain-religion",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+\n",
      "|office_id|\n",
      "+---------+\n",
      "|        1|\n",
      "|        2|\n",
      "|        3|\n",
      "|        4|\n",
      "|        5|\n",
      "+---------+\n",
      "\n",
      "+---------+\n",
      "|office_id|\n",
      "+---------+\n",
      "|1        |\n",
      "|2        |\n",
      "|3        |\n",
      "|4        |\n",
      "|5        |\n",
      "+---------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# dataframe api\n",
    "offices.select(col('id').cast('string').alias('office_id')).show()\n",
    "\n",
    "# spark sql api\n",
    "table_select = spark.sql(\"\"\"\n",
    "                    select cast(id as string) as office_id\n",
    "                    from offices \n",
    "                    \"\"\"\n",
    "                    )\n",
    "table_select.show(50, False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "stunning-surveillance",
   "metadata": {},
   "source": [
    "## Summary Statistics"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "expressed-journal",
   "metadata": {},
   "source": [
    "Note: This isn't something particularly native to do with SQL. In postgres we can use things like crosstab, avg, or median to get something ismilar. Example here only includes usage for dataframe api.\n",
    "\n",
    "**Example Includes**\n",
    "\n",
    "* describe()\n",
    "* show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "induced-cliff",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+------------------+---------+---------------+--------+----------+-----+------------------+\n",
      "|summary|                id|     name|        address|address2|      city|state|           zipcode|\n",
      "+-------+------------------+---------+---------------+--------+----------+-----+------------------+\n",
      "|  count|                 5|        5|              5|       0|         5|    5|                 5|\n",
      "|   mean|               3.0|     null|           null|    null|      null| null|           20267.6|\n",
      "| stddev|1.5811388300841898|     null|           null|    null|      null| null|20479.819769714773|\n",
      "|    min|                 1|Location1| 123 Place Road|    null|    Boston|   MA|             02101|\n",
      "|    max|                 5|Location5|838 Rock Avenue|    null|Pittsburgh|   PA|             55111|\n",
      "+-------+------------------+---------+---------------+--------+----------+-----+------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# dataframe api\n",
    "offices.describe().show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
